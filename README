Try umitools whitelist for barcode extraction rather than cutadapt as will record a csv with the barcode, associated reads 
Maybe first use cutadapt as easier 

Questions:
- proportion of our barcodes that are noise?? num=$(gzip -dc ./230523_0.01_test/clustering/starcode/SRR20707784_starcode_umi_clusters.txt.gz | cut  -f2 | ggrep "[^1]"  | wc -l ); denom=$(gzip -dc ./230523_0.01_test/clustering/starcode/SRR20707784_starcode_umi_clusters.txt.gz | wc -l); echo $num; echo $denom; echo "$num/$denom" | bc -l
  need to run for full sample and perhaps drop clusters with 1 read

-  bed file is 0 based, half =-pne and genbank is 1-based, so have converted bed file
- Why merge reads prior to trimming adapters? for length filtering
- demultiplex script & how to handle I/O bottleneck from number of reads... could we just discard reads that don't match barcode? So much data as is..
- Need to find way to filter reads if the barcodes are shorter than 32bp before processing to speed up
- OUtput that we need
- Could maybe create a barcode file from clustering results?

Outputs:
- BBmerge merging ~80% of reads, 92% with flask, but maybe higher quality results?
- less reads recovered in the umi-tools extract function (~10%)
- Changes Levinstein distance to 4 so less clusters to cycle through

Notes:
- The Tat reference sequence includes the 32bp N for barcode matching
- If demultiplexing, need to first combine all files and then demultiplex
- pipeline took two hours to run with 10k reads for 3x samples... may need to drop some of the processes
- instead of splitting files like this, could we add consensus barcode to read header and retain this information in bam file? if the samples are different, then we could call variants on the bam and just write to csv...


Used info here to create my own custom db
https://www.biostars.org/p/432180/

24/05/23
----
Testing UMI-tools whitelist on the subsampled files to identify cell barcodes. Compare with other clustering tools
If using run on merge rather than trimmed reads
umi_tools whitelist --method 'reads' --knee-method "density" --error-correct-threshold --plot-prefix 'test.out.umitools' --ed-above-threshold 'discard' --extract-method=regex --bc-pattern='.+(?P<discard_1>TGGATCCGGTACCGAGGAGATCTG){s<=2}(?P<cell_1>.{32}){s<=2}(?P<discard_2>GCGATCGC.+)$
Even with full file, only 40% of the reads have more than one member of the


#SNPeff dependencies, configs is stored under /Users/martingordon/anaconda3/envs/gigaassay/share/snpeff-5.1-2
# added HIV-1 to the config file

16/08/23
---
Testing the pipelin with minimal number of reads
Mainly we want to see how the vcf merging works; think we were filtering reads that were incoreect size


25/09/23
----
Issue with nextflow process when generating large number of files: see slack issue for details: https://nextflow.slack.com/archives/C02T98A23U7/p1695673765111919
Solution for now is to compress output in dir and move that around, then iterate through the files in each work dir

04/10/23
----
Run the pipeline with 

TODO
---
Modularise the pipeline

